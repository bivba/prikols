{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-24T07:18:02.408061Z",
     "iopub.status.busy": "2024-12-24T07:18:02.407710Z",
     "iopub.status.idle": "2024-12-24T07:18:02.430377Z",
     "shell.execute_reply": "2024-12-24T07:18:02.429450Z",
     "shell.execute_reply.started": "2024-12-24T07:18:02.408030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a05fa856707460fbe10920b23758138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T08:31:38.094368Z",
     "iopub.status.busy": "2024-12-24T08:31:38.094048Z",
     "iopub.status.idle": "2024-12-24T08:35:33.245839Z",
     "shell.execute_reply": "2024-12-24T08:35:33.244774Z",
     "shell.execute_reply.started": "2024-12-24T08:31:38.094337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install faiss-gpu -q\n",
    "\n",
    "!pip install deep-translator -q\n",
    "\n",
    "!pip install langdetect -q\n",
    "\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip uninstall -y torch torchvision torchaudio xformers\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install sentence_transformers peft\n",
    "\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T08:35:33.247295Z",
     "iopub.status.busy": "2024-12-24T08:35:33.247019Z",
     "iopub.status.idle": "2024-12-24T08:35:51.657570Z",
     "shell.execute_reply": "2024-12-24T08:35:51.656658Z",
     "shell.execute_reply.started": "2024-12-24T08:35:33.247274Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import nltk\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "import pickle\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T08:35:51.659697Z",
     "iopub.status.busy": "2024-12-24T08:35:51.659378Z",
     "iopub.status.idle": "2024-12-24T08:35:54.299644Z",
     "shell.execute_reply": "2024-12-24T08:35:54.298800Z",
     "shell.execute_reply.started": "2024-12-24T08:35:51.659665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'prikols'...\n",
      "remote: Enumerating objects: 104, done.\u001b[K\n",
      "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
      "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
      "remote: Total 104 (delta 44), reused 56 (delta 26), pack-reused 19 (from 2)\u001b[K\n",
      "Receiving objects: 100% (104/104), 43.02 MiB | 38.54 MiB/s, done.\n",
      "Resolving deltas: 100% (48/48), done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!rm -rf prikols\n",
    "!git clone https://github.com/bivba/prikols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T08:35:54.301342Z",
     "iopub.status.busy": "2024-12-24T08:35:54.301046Z",
     "iopub.status.idle": "2024-12-24T08:35:57.774395Z",
     "shell.execute_reply": "2024-12-24T08:35:57.773684Z",
     "shell.execute_reply.started": "2024-12-24T08:35:54.301321Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641a07ddf83f4c6bbd5c5511e090d063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ceccc0319949b1a905083167339aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb31d817e48b4c4e8bf6487a67905136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f950fbe71747fc81ebdc33705c6836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccb4ca4c41b4de4b45e384fe5e1d496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e764a7cab7cb4d72a491208c2cd6ebec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df70bab213b9425488e29e6073218945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34f5f9c8381400690f7cab258b46c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e533c18c7c24f21834561a4edc9684f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1452db4f7d514002aef2f746c97a5e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168fde0f877d421e8700868d81cb15e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "client = GoogleTranslator(source='ru', target='en')\n",
    "\n",
    "text = open('/kaggle/working/prikols/context.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T08:35:57.775681Z",
     "iopub.status.busy": "2024-12-24T08:35:57.775355Z",
     "iopub.status.idle": "2024-12-24T08:40:22.681798Z",
     "shell.execute_reply": "2024-12-24T08:40:22.680852Z",
     "shell.execute_reply.started": "2024-12-24T08:35:57.775651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.8: Fast Qwen2 patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981ae43423e042349e45c77b5e778e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/210k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c51bb3b7241476984e7859c9aa7d486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d0a992d7b44e2dbbdcc11099bbc380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cb5ba02fa941068a8b80e348e29981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c9e1553bc54fc7b321a8ef2d3f4a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c40f6296064e1493880db30cf45c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5e80fa277947fdae97268c169d87ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e038d3340d14c12b319581d2037d4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7967258f82a448cb41efdd0f7415954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c5b4f9a0354b0d89c0b27b4289147b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b5f24d27dc4c1997e107253c7c283f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4e7be4c1ce483eaef4a7e279419561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc66fbdf79cb4a19897778a30e3dc0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/275M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.8 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "max_seq_length = 3072\n",
    "\n",
    "dtype = None\n",
    "\n",
    "load_in_4bit = True\n",
    "\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "\n",
    "    model_name = \"bivba/quant_4bit\",\n",
    "\n",
    "    max_seq_length = max_seq_length,\n",
    "\n",
    "    dtype = dtype,\n",
    "\n",
    "    load_in_4bit = load_in_4bit\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T08:40:22.683476Z",
     "iopub.status.busy": "2024-12-24T08:40:22.683130Z",
     "iopub.status.idle": "2024-12-24T08:40:22.918526Z",
     "shell.execute_reply": "2024-12-24T08:40:22.917801Z",
     "shell.execute_reply.started": "2024-12-24T08:40:22.683444Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "def split_document(document, chunk_size=256, overlap=32):\n",
    "\n",
    "    sentences = sent_tokenize(document)\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk = []\n",
    "\n",
    "    current_length = 0\n",
    "\n",
    "\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        sentence_length = len(sentence)\n",
    "\n",
    "        if current_length + sentence_length <= chunk_size:\n",
    "\n",
    "            current_chunk.append(sentence)\n",
    "\n",
    "            current_length += sentence_length\n",
    "\n",
    "        else:\n",
    "\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "            current_chunk = current_chunk[-overlap:] + [sentence]\n",
    "\n",
    "            current_length = sum(len(s) for s in current_chunk)\n",
    "\n",
    "\n",
    "\n",
    "    if current_chunk:\n",
    "\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T08:40:22.919559Z",
     "iopub.status.busy": "2024-12-24T08:40:22.919316Z",
     "iopub.status.idle": "2024-12-24T08:41:22.419629Z",
     "shell.execute_reply": "2024-12-24T08:41:22.418872Z",
     "shell.execute_reply.started": "2024-12-24T08:40:22.919538Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d78c8e702a049d3aca4d0eb5089bd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/486 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunks = split_document(text)\n",
    "\n",
    "encoded = token.encode(chunks, return_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T08:41:22.422247Z",
     "iopub.status.busy": "2024-12-24T08:41:22.422023Z",
     "iopub.status.idle": "2024-12-24T08:41:22.460779Z",
     "shell.execute_reply": "2024-12-24T08:41:22.460137Z",
     "shell.execute_reply.started": "2024-12-24T08:41:22.422229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "faiss_index = faiss.read_index('prikols/context (1).faiss')\n",
    "\n",
    "def find_context(query, k=2):\n",
    "\n",
    "  if detect(query) == 'ru':\n",
    "\n",
    "    query = client.translate(query)\n",
    "\n",
    "  q_emb = token.encode([query])\n",
    "\n",
    "  q_emb = q_emb.astype(np.float32)\n",
    "\n",
    "  _, indices = faiss_index.search(np.array(q_emb), k)\n",
    "\n",
    "  retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "\n",
    "  context = ' '.join(retrieved_chunks)\n",
    "\n",
    "  return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T08:41:22.462154Z",
     "iopub.status.busy": "2024-12-24T08:41:22.461862Z",
     "iopub.status.idle": "2024-12-24T08:41:22.777158Z",
     "shell.execute_reply": "2024-12-24T08:41:22.776597Z",
     "shell.execute_reply.started": "2024-12-24T08:41:22.462125Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a1f733cec64880ba8c136dc9b716b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "question_prompt = \"\"\"You are an AI assistaint helping with answering questions about quantum physics and quantum computer science. Below is the Question, write a response that appropriately answers the question.\n",
    "\n",
    "\n",
    "### Question:\n",
    "\n",
    "{}\n",
    "\n",
    "\n",
    "### Answer:\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instructions = [client.translate(example) for example in examples['question']]\n",
    "    #inputs       = [find_context(instruction) for instruction in instructions]\n",
    "    outputs      = [client.translate(example) for example in examples['answer']]\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "\n",
    "        text = question_prompt.format(instruction, output) + EOS_TOKEN\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "pass\n",
    "\n",
    "qas = load_dataset('json', data_files='prikols/test.json', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T08:59:03.806592Z",
     "iopub.status.busy": "2024-12-24T08:59:03.806255Z",
     "iopub.status.idle": "2024-12-24T08:59:03.811691Z",
     "shell.execute_reply": "2024-12-24T08:59:03.810797Z",
     "shell.execute_reply.started": "2024-12-24T08:59:03.806563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "test_prompt = \"\"\"\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "        \n",
    "        [\n",
    "        \n",
    "            question_prompt.format(\n",
    "        \n",
    "                client.translate(text),\n",
    "                \n",
    "                '', # output - leave this blank for generation!\n",
    "        \n",
    "            )\n",
    "        \n",
    "        ], return_tensors = \"pt\").to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True, pad_token_id = tokenizer.eos_token_id)\n",
    "        ans = tokenizer.batch_decode(outputs)\n",
    "        del inputs\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T08:41:22.783975Z",
     "iopub.status.busy": "2024-12-24T08:41:22.783690Z",
     "iopub.status.idle": "2024-12-24T08:41:22.797564Z",
     "shell.execute_reply": "2024-12-24T08:41:22.796831Z",
     "shell.execute_reply.started": "2024-12-24T08:41:22.783946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def retrieve(answ):\n",
    "    match = re.search(r\"(### Answer:.*?)(?=###|$)\", answ[0], re.DOTALL)\n",
    "    answer = ''\n",
    "    if match:\n",
    "        answer = match.group(1)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T09:55:40.799714Z",
     "iopub.status.busy": "2024-12-24T09:55:40.799315Z",
     "iopub.status.idle": "2024-12-24T09:56:42.751555Z",
     "shell.execute_reply": "2024-12-24T09:56:42.750726Z",
     "shell.execute_reply.started": "2024-12-24T09:55:40.799683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle ### Answer:\n",
       "\n",
       "1) 00000000\n",
       "2) 00000010\n",
       "3) 00000001\n",
       "4) 00000011\n",
       "\n",
       "The correct answer is 2) 00000010. The B92 protocol is a quantum key distribution protocol that allows two parties, commonly referred to as Alice and Bob, to establish a shared secret key. This key can be used for secure communication, ensuring that any eavesdropping attempt can be detected. The protocol exploits the principles of quantum physics, particularly the Heisenberg Uncertainty Principle, to provide secure key generation. Here is an outline of the B92 protocol:\n",
       "\n",
       "1. Alice prepares quantum states: Alice has a random binary string \\(s_A\\) that she wants to transmit securely to Bob. She encodes this string into a series of quantum states using two non-orthogonal states, typically \\(|0\\rangle\\) and \\(|q_+\\rangle\\), where \\(|q_+\\rangle = \\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}}\\). Specifically, she maps \\(s_A = a_1a_2...a_n\\) to the state:\n",
       "\n",
       "\\[|\\psi\\rangle = (|q_+\\rangle)^{a_1} \\otimes (|q_+\\rangle)^{a_2} \\otimes ... \\otimes (|q_+\\rangle)^{a_n},]\n",
       "\n",
       "where \\(a_i \\in \\{0,1\\}\\). 2. Quantum transmission: Alice sends the generated state \\(|\\psi\\rangle\\) to Bob through a quantum channel. 3. Bob's measurements: Upon receiving the quantum state, Bob measures each qubit in the state using either the basis \\(\\{\\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}}, \\frac{|0\\rangle - |1\\rangle}{\\sqrt{2}}\\}\\) or the basis \\(\\{\\frac{|0\\rangle + i|1\\rangle}{\\sqrt{2}}, \\frac{|0\\rangle - i|1\\rangle}{\\sqrt{2}}\\}\\). He chooses one of the bases randomly for each qubit. 4. Public discussion: After the measurements, Alice and Bob publicly discuss over an open classical communication channel which basis Bob used for each qubit. 5. Key generation: Alice and Bob identify$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Math\n",
    "\n",
    "input_message = 'Для реализации протокола В92 абонент А записал случайную бинарную последовательность вида 01001110 и на ее основе создал регистр в соответствии с требованием протокола В92. Отослал кубиты регистра абоненту Б. Абонент Б составил бинарную последовательность 01010100 и действовал на основании этой последовательности на поступающие кубиты операторами, которые определены в протоколе В92 и  проводил их измерение. Далее он сообщил абоненту А  по открытому каналу связи номера кубит, измерение которых дало значение, равное 1. Отметьте ве те секретные ключи, которые могли сформироваться у абонентов? Выполните необходимые преобразования на листе бумаги и отметьте правильный результат.'\n",
    "\n",
    "Math(retrieve(predict(input_message)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T08:11:03.597236Z",
     "iopub.status.busy": "2024-12-23T08:11:03.596965Z",
     "iopub.status.idle": "2024-12-23T08:11:09.150316Z",
     "shell.execute_reply": "2024-12-23T08:11:09.149214Z",
     "shell.execute_reply.started": "2024-12-23T08:11:03.597216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:26:53.527089Z",
     "iopub.status.busy": "2024-12-23T09:26:53.526754Z",
     "iopub.status.idle": "2024-12-23T09:26:53.531759Z",
     "shell.execute_reply": "2024-12-23T09:26:53.530892Z",
     "shell.execute_reply.started": "2024-12-23T09:26:53.527045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scored = []\n",
    "answers = []\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute(reference, predicted):\n",
    "    scores = scorer.score(reference, predicted)\n",
    "\n",
    "    scored.append(scores['rougeL'][2])\n",
    "    answers.append(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:26:54.318060Z",
     "iopub.status.busy": "2024-12-23T09:26:54.317723Z",
     "iopub.status.idle": "2024-12-23T09:26:55.610390Z",
     "shell.execute_reply": "2024-12-23T09:26:55.609470Z",
     "shell.execute_reply.started": "2024-12-23T09:26:54.318030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mark the correct statements for linear and Hermitian operators \\\\\\\\\\n1) eigenfunctions are real functions \\\\\\\\\\n2) eigenfunctions form a complete set \\\\\\\\\\n3) eigenvalues \\u200b\\u200bform a limited set of ordered numbers \\\\\\\\\\n4) eigenfunctions are orthonormal \\\\\\\\\\n5) eigenvalues \\u200b\\u200bform an equidistant set \\\\\\\\\\n6) eigenvalues \\u200b\\u200bare real numbers \\\\\\\\\\n7) eigenfunctions of commuting operators are the same \\\\\\\\'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.translate(qas['question'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:27:23.273798Z",
     "iopub.status.busy": "2024-12-23T09:27:23.273504Z",
     "iopub.status.idle": "2024-12-23T09:47:52.766080Z",
     "shell.execute_reply": "2024-12-23T09:47:52.765313Z",
     "shell.execute_reply.started": "2024-12-23T09:27:23.273775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [20:29<00:00, 58.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(qas))):\n",
    "    answer = retrieve(predict(qas['question'][i]))\n",
    "    compute(qas['answer'][i], answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:48:03.427374Z",
     "iopub.status.busy": "2024-12-23T09:48:03.427059Z",
     "iopub.status.idle": "2024-12-23T09:48:03.432305Z",
     "shell.execute_reply": "2024-12-23T09:48:03.431578Z",
     "shell.execute_reply.started": "2024-12-23T09:48:03.427351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/working/scores.txt', 'w') as f:\n",
    "    for n, i in enumerate(scored):\n",
    "        f.write(str(n + 1) + '.' +str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:48:05.516235Z",
     "iopub.status.busy": "2024-12-23T09:48:05.515821Z",
     "iopub.status.idle": "2024-12-23T09:48:05.521223Z",
     "shell.execute_reply": "2024-12-23T09:48:05.520346Z",
     "shell.execute_reply.started": "2024-12-23T09:48:05.516197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/working/answers.txt', 'w') as f:\n",
    "    for n, i in enumerate(answers):\n",
    "        f.write(str(n + 1) + '.' + str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:48:15.682273Z",
     "iopub.status.busy": "2024-12-23T09:48:15.681930Z",
     "iopub.status.idle": "2024-12-23T09:48:15.689060Z",
     "shell.execute_reply": "2024-12-23T09:48:15.688282Z",
     "shell.execute_reply.started": "2024-12-23T09:48:15.682246Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='answers.txt' target='_blank'>answers.txt</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/answers.txt"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /kaggle/working\n",
    "from IPython.display import FileLink\n",
    "FileLink('answers.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T09:26:11.521861Z",
     "iopub.status.busy": "2024-12-23T09:26:11.521629Z",
     "iopub.status.idle": "2024-12-23T09:26:11.527118Z",
     "shell.execute_reply": "2024-12-23T09:26:11.526183Z",
     "shell.execute_reply.started": "2024-12-23T09:26:11.521842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='scores.txt' target='_blank'>scores.txt</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/scores.txt"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('scores.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T11:40:14.064547Z",
     "iopub.status.busy": "2024-12-17T11:40:14.064120Z",
     "iopub.status.idle": "2024-12-17T11:40:14.069454Z",
     "shell.execute_reply": "2024-12-17T11:40:14.068379Z",
     "shell.execute_reply.started": "2024-12-17T11:40:14.064505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
